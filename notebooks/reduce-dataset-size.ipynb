{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce dataset size\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os                                  # os handles directory/workspace changes\n",
    "from subprocess import Popen               # Run shell commands\n",
    "import pandas as pd                        # Pandas to load the data initially\n",
    "from tqdm.auto import tqdm                 # Progress bar\n",
    "import yaml                                # Save and load YAML files\n",
    "import random                              # Randomization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Initializing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../../..\")\n",
    "# Path to the parquet dataset files\n",
    "data_path = 'Datasets/Thesis/eICU/cleaned/normalized/ohe/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Cloud settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = input('Bucket name:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_ratio = 0.2                     # Percentage of the data which will be used as a test set\n",
    "validation_ratio = 0.1                     # Percentage of the data from the training set which is used for validation purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduction ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduct_ratio = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting which unit stays end up in death\n",
    "\n",
    "Go one by one in each unit stay's file and build a dictionary which indicates which contain positive samples (i.e. death happening at least 96h after the last sample in the unit stay data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary that indicates, for each unit stay file, if it's positive (i.e. death happening at least 96h after the last sample in the unit stay data) or negative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ends_dead = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(range(8000)):\n",
    "    # Download and read the file\n",
    "    Popen(f'gsutil cp gs://{bucket_name}/eICU_{i}.ftr .', shell=True).wait()\n",
    "    df = pd.read_feather(f'eICU_{i}.ftr')\n",
    "    # Check if it's a positive or negative case (dead or alive)\n",
    "    # 5760 minutes = 96 hours\n",
    "    if df.death_ts.max() <= df.ts.max() + 5760:\n",
    "        ends_dead[f'eICU_{i}.ftr'] = True\n",
    "        print(f'File eICU_{i}.ftr is positive (df.death_ts.max() = df.ts.max() + {df.death_ts.max() - df.ts.max()})')\n",
    "    else:\n",
    "        ends_dead[f'eICU_{i}.ftr'] = False\n",
    "        print(f'File eICU_{i}.ftr is false (df.death_ts.max() = df.ts.max() + {df.death_ts.max() - df.ts.max()})')\n",
    "    # Delete the file from disk\n",
    "    Popen(f'rm -rf eICU_{i}.ftr', shell=True).wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = open(f'{data_path}dead_or_alive.yml', 'w')\n",
    "yaml.dump(ends_dead, stream, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ends_dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = open(f'{data_path}dead_or_alive.yml', 'r')\n",
    "ends_dead = yaml.load(stream, Loader=yaml.FullLoader)\n",
    "ends_dead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating into train, validation and test sets\n",
    "\n",
    "Since I want to reduce the dataset size to 1/10 (i.e. to a total of 800 unit stays) and have each set contain approximately the same death ratio, I'll use the dictionary created above to get 576 training unit stays (0.9x0.8x800), 64 validation unit stays (0.1x0.8x800) and 160 test unit stays (0.2x800)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the positive files and the negative ones into two lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_files = list()\n",
    "negative_files = list()\n",
    "for file, is_positive in ends_dead.items():\n",
    "    if is_positive:\n",
    "        positive_files.append(file)\n",
    "    else:\n",
    "        negative_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_positives = len(positive_files)\n",
    "n_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_negatives = len(negative_files)\n",
    "n_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_label_ratio = n_positives / (n_positives + n_negatives)\n",
    "pos_label_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the required percentages for each set, maintaining the label ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_positive_files = random.sample(positive_files, \n",
    "                                     round(reduct_ratio * (1 - test_train_ratio) * (1 - validation_ratio) * n_positives))\n",
    "train_negative_files = random.sample(negative_files, \n",
    "                                     round(reduct_ratio * (1 - test_train_ratio) * (1 - validation_ratio) * n_negatives))\n",
    "train_files = train_positive_files + train_negative_files\n",
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[positive_files.remove(file) for file in train_positive_files]\n",
    "[negative_files.remove(file) for file in train_negative_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_positive_files = random.sample(positive_files, \n",
    "                                   round(reduct_ratio * (1 - test_train_ratio) * validation_ratio * n_positives))\n",
    "val_negative_files = random.sample(negative_files, \n",
    "                                   round(reduct_ratio * (1 - test_train_ratio) * validation_ratio * n_negatives))\n",
    "val_files = val_positive_files + val_negative_files\n",
    "val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[positive_files.remove(file) for file in val_positive_files]\n",
    "[negative_files.remove(file) for file in val_negative_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_positive_files = random.sample(positive_files, \n",
    "                                    round(reduct_ratio * test_train_ratio * n_positives))\n",
    "test_negative_files = random.sample(negative_files, \n",
    "                                    round(reduct_ratio * test_train_ratio * n_negatives))\n",
    "test_files = test_positive_files + test_negative_files\n",
    "test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[positive_files.remove(file) for file in test_positive_files]\n",
    "[negative_files.remove(file) for file in test_negative_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(train_files) & set(val_files) & set(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get only the files' numbers, instead of their full names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indeces = [int(file_name.split('eICU_')[1].split('.ftr')[0]) for file_name in train_files]\n",
    "train_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_indeces = [int(file_name.split('eICU_')[1].split('.ftr')[0]) for file_name in val_files]\n",
    "val_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indeces = [int(file_name.split('eICU_')[1].split('.ftr')[0]) for file_name in test_files]\n",
    "test_indeces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the train, validation and test indeces (i.e. the lists of file numbers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eICU_tvt_sets = dict(train_indeces=train_indeces, \n",
    "                     val_indeces=val_indeces, \n",
    "                     test_indeces=test_indeces)\n",
    "eICU_tvt_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = open(f'{data_path}eICU_tvt_sets.yml', 'w')\n",
    "yaml.dump(eICU_tvt_sets, stream, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "eICU-mortality-prediction",
   "language": "python",
   "name": "eicu-mortality-prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
