{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data-utils tests\n",
    "---\n",
    "\n",
    "Experimenting with data-utils methods to check if everything is working properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd                # Dask to handle big data in dataframes\n",
    "import pandas as pd                        # Pandas to load the data initially\n",
    "from dask.distributed import Client        # Dask scheduler\n",
    "import os                                  # os handles directory/workspace changes\n",
    "import numpy as np                         # NumPy to handle numeric and NaN operations\n",
    "import torch                               # PyTorch to create and apply deep learning models\n",
    "import data_utils as du                    # Data science and machine learning relevant methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.machine_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging packages\n",
    "import pixiedust                           # Debugging in Jupyter Notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../../..\")\n",
    "\n",
    "# Path to the CSV dataset files\n",
    "data_path = 'Datasets/Others/digidb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up local cluster\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.run(os.getcwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Opting on testing with a [Digimon dataset from Kaggle](https://www.kaggle.com/rtatman/digidb/data#), for nostalgia purposes ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(f'{data_path}DigiDB_digimonlist.csv', dtype={'Digimon': str, 'Stage': str, 'Type': str, 'Attribute': str})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Stage.dtype == 'object'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Attribute'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the column's string values to have the same, standard format\n",
    "tmp_df = du.data_processing.clean_naming(df, feature)\n",
    "tmp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique values of the cateforical feature\n",
    "unique_values = df[feature].unique()\n",
    "if 'dask' in str(type(df)):\n",
    "    # Make sure that the unique values are computed, in case we're using Dask\n",
    "    unique_values = unique_values.compute()\n",
    "# Enumerate the unique values in the categorical feature and put them in a dictionary\n",
    "enum_dict = du.embedding.create_enum_dict(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enum_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a series from the enumerations of the original feature's categories\n",
    "enum_series = df[feature].map(lambda x: du.utils.apply_dict_convertion(x, enum_dict, nan_value=0), meta=('x', int))\n",
    "enum_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Sometimes in Dask, and only sometimes, for some reason this doesn't work, \n",
    "# as it considers that 'Attribute' column has the same unique values as 'Type' Â¯\\_(ãƒ„)_/Â¯\n",
    "# Try to find out what's wrong\n",
    "feat_enum = dict()\n",
    "for feature in df.columns:\n",
    "    if df[feature].dtype == 'object':\n",
    "        # Prepare for embedding, i.e. enumerate categories\n",
    "        df[feature], feat_enum[feature] = du.embedding.enum_categorical_feature(df, feature)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, df, label_name=None):\n",
    "        # Counter that indicates in which column we're in when searching for the label column\n",
    "        col_num = 0\n",
    "        for col in df.columns:\n",
    "            if 'label' in col or col == label_name:\n",
    "                # Column number corresponding to the label\n",
    "                self.label_column = col_num\n",
    "                break\n",
    "            col_num += 1\n",
    "        # Convert the data into a NumPy array\n",
    "        data = df.to_numpy()\n",
    "        # Column numbers corresponding to the features\n",
    "        self.features_columns = list(range(self.label_column)) + list(range(self.label_column + 1, data.shape[1]))\n",
    "        # Features\n",
    "        self.X = data[:, self.features_columns]\n",
    "        # Labels\n",
    "        self.y = data[:, self.label_column]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        x = self.X[item]\n",
    "        y = self.y[item]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(df, 'Attribute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing data utils methods and their reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating in different sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = du.machine_learning.create_train_sets(dataset, get_indeces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_dataloader:\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems to be working great! (at least with the NumPy's random seed; still need to confirm on PyTorch)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "eicu-mortality-prediction",
   "language": "python",
   "name": "eicu-mortality-prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
