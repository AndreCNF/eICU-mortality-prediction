{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models dummy tests\n",
    "\n",
    "Testing models from the project defined classes, including the embedding layers and time intervals handling, on dummy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd                # Dask to handle big data in dataframes\n",
    "import pandas as pd                        # Pandas to load the data initially\n",
    "from dask.distributed import Client        # Dask scheduler\n",
    "import numpy as np                         # Mathematical operations package, allowing also for missing values representation\n",
    "import torch                               # PyTorch for tensor and deep learning operations\n",
    "import data_utils as du                    # Data science and machine learning relevant methods\n",
    "import os                                  # os handles directory/workspace changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust                           # Debugging in Jupyter Notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to scripts directory\n",
    "os.chdir('../../scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Time_Series_Dataset import Time_Series_Dataset # Dataset class that helps fetching batches of data\n",
    "import Models                              # Script with all the machine learning model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to parent directory (presumably \"eICU-mortality-prediction\")\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data that we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "dmy_data = np.array([[0, 0, 23, 284, 70, 5, np.nan, 0],\n",
    "                     [0, 1, 23, 284, 70, 5, 'b', 0],\n",
    "                     [0, 2, 24, 270, 73, 5, 'b', 0],\n",
    "                     [0, 3, 22, 290, 71, 5, 'a', 0],\n",
    "                     [0, 3, 22, 290, 71, 5, 'b', 0],\n",
    "                     [0, 4, 20, 288, 65, 4, 'a', 1],\n",
    "                     [0, 4, 20, 288, 65, 4, 'b', 1],\n",
    "                     [0, 5, 21, 297, 64, 4, 'a', 1],\n",
    "                     [0, 5, 21, 297, 64, 4, 'b', 1],\n",
    "                     [0, 5, 21, 297, 64, 4, 'c', 1],\n",
    "                     [1, 0, 25, 300, 76, 5, 'a', 0],\n",
    "                     [1, 1, 19, 283, 70, 5, 'c', 0],\n",
    "                     [1, 2, 19, 306, 59, 5, 'a', 1],\n",
    "                     [1, 2, 19, 306, 59, 5, 'c', 1],\n",
    "                     [1, 3, 18, 298, 55, 3, 'c', 1],\n",
    "                     [2, 0, 20, 250, 70, 5, 'c', 0],\n",
    "                     [2, 1, 20, 254, 68, 4, 'a', 1],\n",
    "                     [2, 1, 20, 254, 68, 4, 'c', 1],\n",
    "                     [2, 2, 19, 244, 70, 3, 'a', 1],\n",
    "                     [3, 0, 27, 264, 78, 4, 'b', 0],\n",
    "                     [3, 1, 22, 293, 67, 4, 'b', 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_df = pd.DataFrame(dmy_data, columns=['subject_id', 'ts', 'Var0', 'Var1', 'Var2', 'Var3', 'Var4', 'label'])\n",
    "dmy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the columns dtypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_df['subject_id'] = dmy_df['subject_id'].astype(int)\n",
    "dmy_df['ts'] = dmy_df['ts'].astype(int)\n",
    "dmy_df['Var0'] = dmy_df['Var0'].astype(int)\n",
    "dmy_df['Var1'] = dmy_df['Var1'].astype(int)\n",
    "dmy_df['Var2'] = dmy_df['Var2'].astype(int)\n",
    "dmy_df['Var3'] = dmy_df['Var3'].astype(int)\n",
    "dmy_df['Var4'] = dmy_df['Var4'].astype(str)\n",
    "dmy_df['label'] = dmy_df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of used features\n",
    "dmy_cols = list(dmy_df.columns)\n",
    "\n",
    "# Remove features that aren't used by the model to predict the label\n",
    "for unused_feature in ['subject_id', 'ts', 'label']:\n",
    "    dmy_cols.remove(unused_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categories\n",
    "\n",
    "Converting the categorical feature `Var4` into a numeric format, so that it can be used by the neural networks and by embedding layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode each row's categorical value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "dmy_df['Var4'], enum_dict = du.embedding.enum_categorical_feature(dmy_df, feature='Var4')\n",
    "dmy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enum_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the rows and their categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_df = du.embedding.join_categorical_enum(dmy_df, cat_feat='Var4', id_columns=['subject_id', 'ts'])\n",
    "dmy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_df.reset_index().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "dmy_norm_df = du.data_processing.normalize_data(dmy_df.reset_index(), id_columns=['subject_id', 'ts'],\n",
    "                                                embed_columns=['Var4'], see_progress=False)\n",
    "dmy_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_norm_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "Pad the data so that all sequences have the same length (so that it can be converted to a PyTorch tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_value = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len_dict = du.padding.get_sequence_length_dict(dmy_norm_df, id_column='subject_id', ts_column='ts')\n",
    "seq_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = du.padding.dataframe_to_padded_tensor(dmy_norm_df, seq_len_dict=seq_len_dict,\n",
    "                                             id_column='subject_id', padding_value=padding_value)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Time_Series_Dataset(data, dmy_norm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating into train and validation sets\n",
    "\n",
    "Since this notebook is only for experimentation purposes, with a very small dummy dataset, we'll not be using a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32                                 # Number of patients in a mini batch\n",
    "n_epochs = 50                                   # Number of epochs\n",
    "lr = 0.001                                      # Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separation in train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train and validation sets data loaders, which will allow loading batches\n",
    "train_dataloader, val_dataloader, _ = du.machine_learning.create_train_sets(dataset, test_train_ratio=0, \n",
    "                                                                            validation_ratio=0.25,\n",
    "                                                                            batch_size=4, get_indeces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(val_dataloader))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Vanilla LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_layers = 1                                  # Number of LSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Models.VanillaLSTM(n_inputs-3, n_hidden, n_outputs, n_layers, p_dropout)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, model_path='models/',\n",
    "                               padding_value=padding_value, do_test=False, log_comet_ml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, seq_len_dict, dataloader=val_dataloader, \n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   padding_value=padding_value, output_rounded=False, \n",
    "                                                   set_name='test', cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                                    for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with embedding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_layers = 1                                  # Number of LSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(enum_dict.items())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Models.VanillaLSTM(n_inputs-3, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                           embed_features=du.search_explore.find_col_idx(dmy_norm_df, 'Var4'), num_embeddings=5,\n",
    "                           embedding_dim=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, model_path='models/',\n",
    "                               padding_value=padding_value, do_test=False, log_comet_ml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, seq_len_dict, dataloader=val_dataloader, \n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   padding_value=padding_value, output_rounded=False, \n",
    "                                                   set_name='test', cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                                    for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with embedding layers and time interval handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepCare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eicu-mortality-prediction",
   "language": "python",
   "name": "eicu-mortality-prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
