{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Models dummy tests\n",
    "---\n",
    "\n",
    "Testing models from the project defined classes, including the embedding layers and time intervals handling, on dummy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import comet_ml                            # Comet.ml can log training metrics, parameters, do version control and parameter optimization\n",
    "import os                                  # os handles directory/workspace changes\n",
    "import pandas as pd                        # Pandas to load the data initially\n",
    "# import modin.pandas as pd                  # Optimized distributed version of Pandas\n",
    "import numpy as np                         # Mathematical operations package, allowing also for missing values representation\n",
    "import torch                               # PyTorch for tensor and deep learning operations\n",
    "import plotly.graph_objs as go             # Plotly for interactive and pretty plots\n",
    "import data_utils as du                    # Data science and machine learning relevant methods\n",
    "from model_interpreter.model_interpreter import ModelInterpreter  # Model interpretability class\n",
    "import shap                                # Model-agnostic interpretability package inspired on Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.set_pandas_library(lib='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pixiedust                           # Debugging in Jupyter Notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Change to scripts directory\n",
    "os.chdir('../../scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import Models                              # Script with all the machine learning model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Change to parent directory (presumably \"eICU-mortality-prediction\")\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Initializing variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comet ML settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_ml_project_name = input('Comet ML project name:')\n",
    "comet_ml_workspace = input('Comet ML workspace:')\n",
    "comet_ml_api_key = getpass.getpass('Comet ML API key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Data that we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "dmy_data = np.array([[0, 0, 23, 284, 70, 5, np.nan, 0],\n",
    "                     [0, 1, 23, 284, 70, 5, 'b', 0],\n",
    "                     [0, 2, 24, 270, 73, 5, 'b', 0],\n",
    "                     [0, 3, 22, 290, 71, 5, 'a', 0],\n",
    "                     [0, 3, 22, 290, 71, 5, 'b', 0],\n",
    "                     [0, 4, 20, 288, 65, 4, 'a', 1],\n",
    "                     [0, 4, 20, 288, 65, 4, 'b', 1],\n",
    "                     [0, 5, 21, 297, 64, 4, 'a', 1],\n",
    "                     [0, 5, 21, 297, 64, 4, 'b', 1],\n",
    "                     [0, 5, 21, 297, 64, 4, 'c', 1],\n",
    "                     [1, 0, 25, 300, 76, 5, 'a', 0],\n",
    "                     [1, 1, 19, 283, 70, 5, 'c', 0],\n",
    "                     [1, 2, 19, 306, 59, 5, 'a', 1],\n",
    "                     [1, 2, 19, 306, 59, 5, 'c', 1],\n",
    "                     [1, 3, 18, 298, 55, 3, 'c', 1],\n",
    "                     [2, 0, 20, 250, 70, 5, 'c', 0],\n",
    "                     [2, 1, 20, 254, 68, 4, 'a', 1],\n",
    "                     [2, 1, 20, 254, 68, 4, 'c', 1],\n",
    "                     [2, 2, 19, 244, 70, 3, 'a', 1],\n",
    "                     [3, 0, 27, 264, 78, 4, 'b', 0],\n",
    "                     [3, 1, 22, 293, 67, 4, 'b', 1],\n",
    "                     [4, 0, 28, 290, 73, 5, 'b', 0],\n",
    "                     [4, 1, 29, 288, 75, 5, 'b', 0],\n",
    "                     [4, 2, 28, 289, 75, 5, 'b', 0],\n",
    "                     [4, 5, 26, 290, 62, 5, 'b', 0],\n",
    "                     [4, 6, 25, 285, 63, 4, 'b', 0],\n",
    "                     [4, 12, 23, 280, 58, 4, 'b', 0],\n",
    "                     [4, 12, 23, 280, 58, 4, 'c', 0],\n",
    "                     [4, 14, 21, 282, 59, 3, 'a', 0],\n",
    "                     [4, 14, 21, 282, 59, 3, 'b', 0],\n",
    "                     [4, 14, 21, 282, 59, 3, 'c', 0],\n",
    "                     [4, 15, 22, 277, 56, 2, 'a', 1],\n",
    "                     [4, 16, 20, 270, 53, 2, 'a', 1],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df = pd.DataFrame(dmy_data, columns=['subject_id', 'ts', 'Var0', 'Var1', 'Var2', 'Var3', 'Var4', 'label'])\n",
    "dmy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Fix the columns dtypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df['subject_id'] = dmy_df['subject_id'].astype(int)\n",
    "dmy_df['ts'] = dmy_df['ts'].astype(int)\n",
    "dmy_df['Var0'] = dmy_df['Var0'].astype(int)\n",
    "dmy_df['Var1'] = dmy_df['Var1'].astype(int)\n",
    "dmy_df['Var2'] = dmy_df['Var2'].astype(int)\n",
    "dmy_df['Var3'] = dmy_df['Var3'].astype(int)\n",
    "dmy_df['Var4'] = dmy_df['Var4'].astype(str)\n",
    "dmy_df['label'] = dmy_df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# List of used features\n",
    "dmy_cols = list(dmy_df.columns)\n",
    "# Remove features that aren't used by the model to predict the label\n",
    "for unused_feature in ['subject_id', 'ts', 'label']:\n",
    "    dmy_cols.remove(unused_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df['subject_id'] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.index[dmy_df['subject_id'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.iloc[dmy_df.index[dmy_df['subject_id'] == 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.set_index(['subject_id', 'ts'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "type(dmy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define if the notebook will run hyperparameter optimization on each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_hyperparam_optim = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Encoding categories\n",
    "\n",
    "Converting the categorical feature `Var4` into one hot encoded columns, so that it can be used by the neural networks and by embedding layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "~Encode each row's categorical value:~\n",
    "\n",
    "One hot encode the categorical feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# dmy_df['Var4'], enum_dict = du.embedding.enum_categorical_feature(dmy_df, feature='Var4',\n",
    "#                                                                   nan_value=0, forbidden_digit=0)\n",
    "# dmy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x1 = pd.get_dummies(dmy_df, columns=['Var4'])\n",
    "x1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x2 = pd.get_dummies(dmy_df, columns=['Var4'], sparse=True)\n",
    "x2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df, ohe_columns = du.data_processing.one_hot_encoding_dataframe(dmy_df, columns='Var4', \n",
    "                                                                    join_rows=False, \n",
    "                                                                    get_new_column_names=True, \n",
    "                                                                    inplace=True)\n",
    "dmy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ohe_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Joining the rows that have the same identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df = du.embedding.join_repeated_rows(dmy_df, id_columns=['subject_id', 'ts'])\n",
    "dmy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Testing the merge of boolean columns\n",
    "tmp_df = dmy_df.rename(columns={'Var4_a': 'Var4_x', 'Var4_b': 'Var4_y'})\n",
    "tmp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.data_processing.merge_columns(tmp_df, cols_to_merge='Var4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Normalizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_norm_df, mean, std = du.data_processing.normalize_data(dmy_df, id_columns=['subject_id', 'ts'],\n",
    "                                                           see_progress=False, get_stats=True)\n",
    "dmy_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# dmy_norm_df, mean, std = du.data_processing.normalize_data(dmy_df, id_columns=['subject_id', 'ts'],\n",
    "#                                                            categ_columns=['Var4'], see_progress=False,\n",
    "#                                                            get_stats=True)\n",
    "# dmy_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# dmy_norm_df, mean, std = du.data_processing.normalize_data(dmy_df, id_columns=['subject_id', 'ts'],\n",
    "#                                                            columns_to_normalize=False,\n",
    "#                                                            columns_to_normalize_categ=('Var4', ['Var0', 'Var1', 'Var2', 'Var3']), \n",
    "#                                                            see_progress=False, get_stats=True)\n",
    "# dmy_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# dmy_norm_df, mean, std = du.data_processing.normalize_data(dmy_df, id_columns=['subject_id', 'ts'],\n",
    "#                                                            columns_to_normalize=False,\n",
    "#                                                            columns_to_normalize_categ=('Var4', 'Var0'), \n",
    "#                                                            see_progress=False, get_stats=True)\n",
    "# dmy_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stats = dict()\n",
    "for key, _ in mean.items():\n",
    "    stats[key] = dict()\n",
    "    stats[key]['mean'] = mean[key]\n",
    "    stats[key]['std'] = std[key]\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_norm_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Padding\n",
    "\n",
    "Pad the data so that all sequences have the same length (so that it can be converted to a PyTorch tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "padding_value = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "seq_len_dict = du.padding.get_sequence_length_dict(dmy_norm_df, id_column='subject_id', ts_column='ts')\n",
    "seq_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "data = du.padding.dataframe_to_padded_tensor(dmy_norm_df, seq_len_dict=seq_len_dict,\n",
    "                                             id_column='subject_id', padding_value=padding_value)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_perm = data.permute(1, 0, 2)\n",
    "data_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_perm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_perm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset = du.datasets.Time_Series_Dataset(dmy_norm_df, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Separating into train and validation sets\n",
    "\n",
    "Since this notebook is only for experimentation purposes, with a very small dummy dataset, we'll not be using a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "batch_size = 32                                 # Number of patients in a mini batch\n",
    "n_epochs = 100                                  # Number of epochs\n",
    "lr = 0.001                                      # Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Separation in train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Get the train and validation sets data loaders, which will allow loading batches\n",
    "train_dataloader, val_dataloader, _ = du.machine_learning.create_train_sets(dataset, test_train_ratio=0, \n",
    "                                                                            validation_ratio=0.25,\n",
    "                                                                            batch_size=4, get_indeces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(iter(val_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Models testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Vanilla LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n",
    "\n",
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_layers = 2                                  # Number of LSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.VanillaLSTM(n_inputs-3, n_hidden, n_outputs, n_layers, p_dropout)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               padding_value=padding_value, do_test=False, log_comet_ml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader, \n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value, \n",
    "                                                   output_rounded=False, set_name='test', \n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### LSTM with embedding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_layers = 2                                  # Number of LSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout\n",
    "embed_features = [du.search_explore.find_col_idx(dmy_norm_df, col) for col in ohe_columns] # Indeces fo the features to be emebedded\n",
    "embed_features.sort()\n",
    "embedding_dim = 2                             # Number of outputs of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "embed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Models.VanillaLSTM(n_inputs-3, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                           embed_features=embed_features, embedding_dim=embedding_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.n_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(model.lstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# model = du.deep_learning.train(model, train_dataloader_df, val_dataloader_df, seq_len_dict=seq_len_dict,\n",
    "#                                batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "#                                padding_value=padding_value, do_test=False, log_comet_ml=False,\n",
    "#                                already_embedded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               padding_value=padding_value, do_test=False, log_comet_ml=False,\n",
    "                               already_embedded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.lstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader, \n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value, \n",
    "                                                   output_rounded=False, set_name='test', \n",
    "                                                   already_embedded=False,\n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### LSTM with embedding layers and time interval handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Adding the time difference feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df['delta_ts'] = dmy_df.groupby('subject_id').ts.diff()\n",
    "dmy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Normalizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "dmy_norm_df = du.data_processing.normalize_data(dmy_df, id_columns=['subject_id', 'ts'],\n",
    "                                                see_progress=False)\n",
    "dmy_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_norm_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Imputation\n",
    "\n",
    "Replace the missing time difference values with the mean (zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_norm_df = du.data_processing.missing_values_imputation(dmy_norm_df, method='zero')\n",
    "dmy_norm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Padding\n",
    "\n",
    "Pad the data so that all sequences have the same length (so that it can be converted to a PyTorch tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "padding_value = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "seq_len_dict = du.padding.get_sequence_length_dict(dmy_norm_df, id_column='subject_id', ts_column='ts')\n",
    "seq_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "data = du.padding.dataframe_to_padded_tensor(dmy_norm_df, seq_len_dict=seq_len_dict,\n",
    "                                             id_column='subject_id', padding_value=padding_value)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset = du.datasets.Time_Series_Dataset(dmy_norm_df, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Separating into train and validation sets\n",
    "\n",
    "Since this notebook is only for experimentation purposes, with a very small dummy dataset, we'll not be using a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "batch_size = 32                                 # Number of patients in a mini batch\n",
    "n_epochs = 100                                  # Number of epochs\n",
    "lr = 0.001                                      # Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Separation in train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Get the train and validation sets data loaders, which will allow loading batches\n",
    "train_dataloader, val_dataloader, _ = du.machine_learning.create_train_sets(dataset, test_train_ratio=0, \n",
    "                                                                            validation_ratio=0.25,\n",
    "                                                                            batch_size=4, get_indeces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "val_features, val_labels = next(iter(val_dataloader))\n",
    "val_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_layers = 2                                  # Number of LSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout\n",
    "embed_features = [du.search_explore.find_col_idx(dmy_norm_df, col) for col in ohe_columns] # Indeces fo the features to be emebedded\n",
    "embed_features.sort()\n",
    "embedding_dim = 2                             # Number of outputs of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = Models.VanillaLSTM(n_inputs-3, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                           embed_features=embed_features, embedding_dim=embedding_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               padding_value=padding_value, do_test=False, log_comet_ml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader, \n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value, \n",
    "                                                   output_rounded=False, set_name='test', \n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### T-LSTM\n",
    "\n",
    "Implementation of the [_Patient Subtyping via Time-Aware LSTM Networks_](http://biometrics.cse.msu.edu/Publications/MachineLearning/Baytasetal_PatientSubtypingViaTimeAwareLSTMNetworks.pdf) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_rnn_layers = 4                              # Number of TLSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout\n",
    "embed_features = [du.search_explore.find_col_idx(dmy_norm_df, col) for col in ohe_columns] # Indeces fo the features to be emebedded\n",
    "embed_features.sort()\n",
    "embedding_dim = 2                             # Number of outputs of the embedding layr\n",
    "# delta_ts_col = du.search_explore.find_col_idx(dmy_norm_df, 'delta_ts')   # Number of the delta_ts column\n",
    "elapsed_time = 'small'                                                   # Indicates if the elapsed time between events is small or long; influences how to discount elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_norm_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "embed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Models.TLSTM(n_inputs-4, n_hidden, n_outputs, n_rnn_layers, p_dropout,\n",
    "                     embed_features=embed_features, embedding_dim=embedding_dim, \n",
    "                     elapsed_time=elapsed_time)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.weight_ih.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[1].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               padding_value=padding_value, do_test=False, log_comet_ml=False,\n",
    "                               is_custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader, \n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value, \n",
    "                                                   output_rounded=False, set_name='test',\n",
    "                                                   is_custom=True,\n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.TLSTM, df=dmy_norm_df, \n",
    "                                                                              config_name='TLSTM_hyperparameter_optimization_config.yaml', \n",
    "                                                                              comet_ml_api_key=comet_ml_api_key,\n",
    "                                                                              comet_ml_project_name=comet_ml_project_name, \n",
    "                                                                              comet_ml_workspace=comet_ml_workspace, \n",
    "                                                                              n_inputs=n_inputs-4, id_column='subject_id',  \n",
    "                                                                              label_column='label', inst_column='ts',\n",
    "                                                                              n_outputs=1, model_type='multivariate_rnn',\n",
    "                                                                              is_custom=True, models_path='models/', array_param=None,\n",
    "                                                                              config_path='notebooks/sandbox/', var_seq=True, \n",
    "                                                                              clip_value=0.5, padding_value=padding_value, \n",
    "                                                                              batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                              lr=lr, test_train_ratio=0, validation_ratio=0.25,\n",
    "                                                                              comet_ml_save_model=True, embed_features=embed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### MF1-LSTM\n",
    "\n",
    "Implementation of the [_Predicting healthcare trajectories from medical records: A deep learning approach_](https://doi.org/10.1016/j.jbi.2017.04.001) paper, time decay version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_rnn_layers = 4                              # Number of TLSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout\n",
    "embed_features = [du.search_explore.find_col_idx(dmy_norm_df, col) for col in ohe_columns] # Indeces fo the features to be emebedded\n",
    "embed_features.sort()\n",
    "embedding_dim = 2                             # Number of outputs of the embedding layr\n",
    "# delta_ts_col = du.search_explore.find_col_idx(dmy_norm_df, 'delta_ts')   # Number of the delta_ts column\n",
    "elapsed_time = 'small'                                                   # Indicates if the elapsed time between events is small or long; influences how to discount elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_norm_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "embed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.MF1LSTM(n_inputs-4, n_hidden, n_outputs, n_rnn_layers, p_dropout,\n",
    "                       embed_features=embed_features, embedding_dim=embedding_dim, \n",
    "                       elapsed_time=elapsed_time)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.weight_ih.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[1].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               ModelClass=Models.MF1LSTM, padding_value=padding_value, do_test=False, \n",
    "                               log_comet_ml=False, is_custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader, \n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value, \n",
    "                                                   output_rounded=False, set_name='test',\n",
    "                                                   is_custom=True, \n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.MF1LSTM, df=dmy_norm_df, \n",
    "                                                                              config_name='TLSTM_hyperparameter_optimization_config.yaml', \n",
    "                                                                              comet_ml_api_key=comet_ml_api_key,\n",
    "                                                                              comet_ml_project_name=comet_ml_project_name, \n",
    "                                                                              comet_ml_workspace=comet_ml_workspace, \n",
    "                                                                              n_inputs=n_inputs-4, id_column='subject_id',  \n",
    "                                                                              label_column='label', inst_column='ts',\n",
    "                                                                              n_outputs=1, model_type='multivariate_rnn',\n",
    "                                                                              is_custom=True, models_path='models/', array_param=None,\n",
    "                                                                              config_path='notebooks/sandbox/', var_seq=True, \n",
    "                                                                              clip_value=0.5, padding_value=padding_value, \n",
    "                                                                              batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                              lr=lr, test_train_ratio=0, validation_ratio=0.25,\n",
    "                                                                              comet_ml_save_model=True, embed_features=embed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### MF2-LSTM\n",
    "\n",
    "Implementation of the [_Predicting healthcare trajectories from medical records: A deep learning approach_](https://doi.org/10.1016/j.jbi.2017.04.001) paper, parametric time version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_rnn_layers = 4                              # Number of TLSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout\n",
    "embed_features = [du.search_explore.find_col_idx(dmy_norm_df, col) for col in ohe_columns] # Indeces fo the features to be emebedded\n",
    "embed_features.sort()\n",
    "embedding_dim = 2                             # Number of outputs of the embedding layr\n",
    "# delta_ts_col = du.search_explore.find_col_idx(dmy_norm_df, 'delta_ts')   # Number of the delta_ts column\n",
    "elapsed_time = 'small'                                                   # Indicates if the elapsed time between events is small or long; influences how to discount elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_norm_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "embed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.MF2LSTM(n_inputs-4, n_hidden, n_outputs, n_rnn_layers, p_dropout,\n",
    "                       embed_features=embed_features, embedding_dim=embedding_dim, \n",
    "                       elapsed_time=elapsed_time)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.weight_ih.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[1].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               ModelClass=Models.MF2LSTM, padding_value=padding_value, do_test=False,\n",
    "                               log_comet_ml=False, is_custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader, \n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value, \n",
    "                                                   output_rounded=False, set_name='test',\n",
    "                                                   is_custom=True,\n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.MF2LSTM, df=dmy_norm_df, \n",
    "                                                                              config_name='TLSTM_hyperparameter_optimization_config.yaml', \n",
    "                                                                              comet_ml_api_key=comet_ml_api_key,\n",
    "                                                                              comet_ml_project_name=comet_ml_project_name, \n",
    "                                                                              comet_ml_workspace=comet_ml_workspace, \n",
    "                                                                              n_inputs=n_inputs-4, id_column='subject_id',  \n",
    "                                                                              label_column='label', inst_column='ts',\n",
    "                                                                              n_outputs=1, model_type='multivariate_rnn',\n",
    "                                                                              is_custom=True, models_path='models/', array_param=None,\n",
    "                                                                              config_path='notebooks/sandbox/', var_seq=True, \n",
    "                                                                              clip_value=0.5, padding_value=padding_value, \n",
    "                                                                              batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                              lr=lr, test_train_ratio=0, validation_ratio=0.25,\n",
    "                                                                              comet_ml_save_model=True, embed_features=embed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "interpreter = ModelInterpreter(model, dmy_norm_df, model_type='multivariate_rnn',\n",
    "                               id_column=0, inst_column=1, fast_calc=True, SHAP_bkgnd_samples=10000,\n",
    "                               random_seed=du.random_seed, padding_value=padding_value, is_custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = np.concatenate([train_features, val_features])\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = np.concatenate([train_labels, val_labels])\n",
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "_ = interpreter.interpret_model(test_data=all_features, \n",
    "                                test_labels=all_labels, instance_importance=True, \n",
    "                                feature_importance='shap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interpreter.feat_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.feat_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.test_data[:, :, 2:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = list(dmy_df.columns)\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = column_names.copy()\n",
    "features_names.remove('subject_id')\n",
    "features_names.remove('ts')\n",
    "features_names.remove('label')\n",
    "features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_column_names = [f'{feature}_shap' for feature in features_names]\n",
    "shap_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n_shap = np.concatenate([interpreter.test_data.numpy(), interpreter.feat_scores], axis=2)\n",
    "data_n_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n_shap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['subject_id', 'ts']+features_names+shap_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n_shap.reshape(-1, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n_shap_columns = ['subject_id', 'ts']+features_names+shap_column_names\n",
    "data_n_shap_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[feature for feature in data_n_shap_columns if feature.endswith('_shap')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_n_shap_df = pd.DataFrame(data=data_n_shap.reshape(-1, 18), columns=data_n_shap_columns)\n",
    "data_n_shap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n_shap_df.to_csv('notebooks/sandbox/dummy_data/data_n_shap_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "du.visualization.shap_summary_plot(interpreter.feat_scores, features_names, max_display=3,\n",
    "                                   background_color='#282828',\n",
    "                                   output_type='plotly',\n",
    "                                   font_family='Roboto', font_size=14,\n",
    "                                   font_color='#ADAFAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.feat_scores.sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.explainer.expected_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interpreter.feat_scores.sum(axis=2) + interpreter.explainer.expected_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interpreter.feat_scores.sum(axis=2)[idx] + interpreter.explainer.expected_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.test_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model(interpreter.test_data[idx, :, 2:].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.test_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.explainer.subject_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interpreter.feat_scores.reshape(-1, model.n_inputs+1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features[:, :4, 2:].numpy().reshape(-1, model.n_inputs+1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effects of all the features\n",
    "shap.summary_plot(interpreter.feat_scores.reshape(-1, model.n_inputs+1), \n",
    "                  features=interpreter.test_data[:, :4, 2:].numpy().reshape(-1, model.n_inputs+1), \n",
    "                  feature_names=interpreter.feat_names, plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TODO] Do the same bar plot as above but in plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(interpreter.feat_scores).reshape(-1, interpreter.feat_scores.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_abs_shap = np.mean(np.abs(interpreter.feat_scores).reshape(-1, interpreter.feat_scores.shape[-1]), axis=0)\n",
    "mean_abs_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = np.argsort(mean_abs_shap)\n",
    "sorted_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interpreter.feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[interpreter.feat_names[idx] for idx in sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_abs_shap[sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure={\n",
    "    'data': [dict(\n",
    "        type='bar',\n",
    "        x=mean_abs_shap[sorted_idx],\n",
    "        y=[interpreter.feat_names[idx] for idx in sorted_idx],\n",
    "        orientation='h'\n",
    "    )],\n",
    "    'layout': dict(\n",
    "        margin=dict(l=0, r=0, t=0, b=0, pad=0),\n",
    "        xaxis_title='mean(|SHAP value|) (average impact on model output magnitude)',\n",
    "        font=dict(\n",
    "                family='Roboto',\n",
    "                size=14,\n",
    "                color='black'\n",
    "            )\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "go.Figure(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.visualization.shap_summary_plot(interpreter.feat_scores, interpreter.feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choosing which example to use\n",
    "# subject_id = 125\n",
    "# patient = utils.find_subject_idx(test_features_denorm, subject_id=subject_id)\n",
    "# patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # True sequence length of the current patient's data\n",
    "# seq_len = seq_len_dict[test_features_denorm[patient, 0, 0].item()]\n",
    "# # Plot the explanation of the predictions for one patient\n",
    "# shap.force_plot(interpreter.explainer.expected_value[0], \n",
    "#                 interpreter.feat_scores[patient, :seq_len], \n",
    "#                 features=test_features_denorm[patient, :seq_len, 2:].numpy(), \n",
    "#                 feature_names=ALS_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Init the JS visualization code\n",
    "# shap.initjs()\n",
    "\n",
    "# # Choosing which timestamp to use\n",
    "# ts = 9\n",
    "\n",
    "# # Plot the explanation of one prediction\n",
    "# shap.force_plot(interpreter.explainer.expected_value[0], \n",
    "#                 interpreter.feat_scores[patient][ts], \n",
    "#                 features=test_features_denorm[patient, ts, 2:].numpy(), \n",
    "#                 feature_names=ALS_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = 0\n",
    "sample = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f'{feature}={val:.2e}' for (feature, val) in zip(interpreter.feat_names, interpreter.test_data[pred, sample, 2:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interpreter.explainer.expected_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.feat_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interpreter.feat_scores[pred, sample].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(interpreter.feat_scores[pred, sample].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.feat_scores[pred, sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(interpreter.test_data[pred, sample, 2:].unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(interpreter.feat_scores[pred, sample]) + interpreter.explainer.expected_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.test_data[pred, sample, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.waterfall_plot(interpreter.explainer.expected_value[0], \n",
    "                    interpreter.feat_scores[pred, sample],\n",
    "                    features=interpreter.test_data[pred, sample, 2:].numpy(), \n",
    "                    feature_names=interpreter.feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.waterfall_plot(interpreter.explainer.expected_value[0], \n",
    "                    interpreter.feat_scores[pred, sample],\n",
    "                    features=interpreter.test_data[pred, sample, 2:].numpy(), \n",
    "                    feature_names=interpreter.feat_names,\n",
    "                    max_display=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# du.visualization.shap_waterfall_plot(interpreter.explainer.expected_value[0], interpreter.feat_scores[pred, sample],\n",
    "du.visualization.shap_waterfall_plot(0, interpreter.feat_scores[pred, sample],\n",
    "                                     interpreter.test_data[pred, sample, 2:], interpreter.feat_names,\n",
    "                                     max_display=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Waterfall(\n",
    "    y = [[\"initial\", \"q1\", \"q2\", \"q3\", \"total\", \"q1\", \"q2\", \"q3\", \"total\"]],\n",
    "    measure = [\"absolute\", \"relative\", \"relative\", \"relative\", \"total\", \"relative\", \"relative\", \"relative\", \"total\"],\n",
    "    x = [1, 2, 3, -1, None, 1, 2, -4, None],\n",
    "    base = 1000,\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Waterfall(\n",
    "    y = [[\"2016\", \"2017\", \"2017\", \"2017\", \"2017\", \"2018\", \"2018\", \"2018\", \"2018\"],\n",
    "        [\"initial\", \"q1\", \"q2\", \"q3\", \"total\", \"q1\", \"q2\", \"q3\", \"total\"]],\n",
    "    measure = [\"absolute\", \"relative\", \"relative\", \"relative\", \"total\", \"relative\", \"relative\", \"relative\", \"total\"],\n",
    "    x = [1.1, 2.2, 3.3, -1.1, None, 1.1, 2.2, -4.4, None],\n",
    "    base = 1000,\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    waterfallgroupgap = 0.5,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Waterfall(\n",
    "    y = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\"],\n",
    "#     measure = [\"absolute\", \"relative\", \"relative\", \"relative\", \"total\", \"relative\", \"relative\", \"relative\", \"total\"],\n",
    "    x = [1, 2, 3, -1, None, 1, 2, -4, None],\n",
    "    base = 1000,\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.feat_scores[pred, sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Waterfall(\n",
    "    y = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'],\n",
    "    x = [1, 2, 1, -2, -1, 3, -4, 1],\n",
    "    base = 100,\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Waterfall(\n",
    "    y = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'],\n",
    "    x = [-1, -2, -1, 2, 1, -3, 4, -1],\n",
    "    base = 100,\n",
    "    orientation='h'\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Deep Care with parametric time\n",
    "\n",
    "Implementation of the [_Predicting healthcare trajectories from medical records: A deep learning approach_](https://doi.org/10.1016/j.jbi.2017.04.001) paper, full parametric time version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:light",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "eICU-mortality-prediction",
   "language": "python",
   "name": "eicu-mortality-prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
