{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Models dummy tests\n",
    "---\n",
    "\n",
    "Testing models from the project defined classes, including the embedding layers and time intervals handling, on dummy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd                        # Pandas to load the data initially\n",
    "# import modin.pandas as pd                  # Optimized distributed version of Pandas\n",
    "import numpy as np                         # Mathematical operations package, allowing also for missing values representation\n",
    "import torch                               # PyTorch for tensor and deep learning operations\n",
    "import data_utils as du                    # Data science and machine learning relevant methods\n",
    "import os                                  # os handles directory/workspace changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.set_pandas_library(lib='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pixiedust                           # Debugging in Jupyter Notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Change to scripts directory\n",
    "os.chdir('../../scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import Models                              # Script with all the machine learning model classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Change to parent directory (presumably \"eICU-mortality-prediction\")\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Initializing variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Data that we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "dmy_data = np.array([[0, 0, 23, 284, 70, 5, np.nan, 0],\n",
    "                     [0, 1, 23, 284, 70, 5, 'b', 0],\n",
    "                     [0, 2, 24, 270, 73, 5, 'b', 0],\n",
    "                     [0, 3, 22, 290, 71, 5, 'a', 0],\n",
    "                     [0, 3, 22, 290, 71, 5, 'b', 0],\n",
    "                     [0, 4, 20, 288, 65, 4, 'a', 1],\n",
    "                     [0, 4, 20, 288, 65, 4, 'b', 1],\n",
    "                     [0, 5, 21, 297, 64, 4, 'a', 1],\n",
    "                     [0, 5, 21, 297, 64, 4, 'b', 1],\n",
    "                     [0, 5, 21, 297, 64, 4, 'c', 1],\n",
    "                     [1, 0, 25, 300, 76, 5, 'a', 0],\n",
    "                     [1, 1, 19, 283, 70, 5, 'c', 0],\n",
    "                     [1, 2, 19, 306, 59, 5, 'a', 1],\n",
    "                     [1, 2, 19, 306, 59, 5, 'c', 1],\n",
    "                     [1, 3, 18, 298, 55, 3, 'c', 1],\n",
    "                     [2, 0, 20, 250, 70, 5, 'c', 0],\n",
    "                     [2, 1, 20, 254, 68, 4, 'a', 1],\n",
    "                     [2, 1, 20, 254, 68, 4, 'c', 1],\n",
    "                     [2, 2, 19, 244, 70, 3, 'a', 1],\n",
    "                     [3, 0, 27, 264, 78, 4, 'b', 0],\n",
    "                     [3, 1, 22, 293, 67, 4, 'b', 1],\n",
    "                     [4, 0, 28, 290, 73, 5, 'b', 0],\n",
    "                     [4, 1, 29, 288, 75, 5, 'b', 0],\n",
    "                     [4, 2, 28, 289, 75, 5, 'b', 0],\n",
    "                     [4, 5, 26, 290, 62, 5, 'b', 0],\n",
    "                     [4, 6, 25, 285, 63, 4, 'b', 0],\n",
    "                     [4, 12, 23, 280, 58, 4, 'b', 0],\n",
    "                     [4, 12, 23, 280, 58, 4, 'c', 0],\n",
    "                     [4, 14, 21, 282, 59, 3, 'a', 0],\n",
    "                     [4, 14, 21, 282, 59, 3, 'b', 0],\n",
    "                     [4, 14, 21, 282, 59, 3, 'c', 0],\n",
    "                     [4, 15, 22, 277, 56, 2, 'a', 1],\n",
    "                     [4, 16, 20, 270, 53, 2, 'a', 1],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df = pd.DataFrame(dmy_data, columns=['subject_id', 'ts', 'Var0', 'Var1', 'Var2', 'Var3', 'Var4', 'label'])\n",
    "dmy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Fix the columns dtypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df['subject_id'] = dmy_df['subject_id'].astype(int)\n",
    "dmy_df['ts'] = dmy_df['ts'].astype(int)\n",
    "dmy_df['Var0'] = dmy_df['Var0'].astype(int)\n",
    "dmy_df['Var1'] = dmy_df['Var1'].astype(int)\n",
    "dmy_df['Var2'] = dmy_df['Var2'].astype(int)\n",
    "dmy_df['Var3'] = dmy_df['Var3'].astype(int)\n",
    "dmy_df['Var4'] = dmy_df['Var4'].astype(str)\n",
    "dmy_df['label'] = dmy_df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# List of used features\n",
    "dmy_cols = list(dmy_df.columns)\n",
    "# Remove features that aren't used by the model to predict the label\n",
    "for unused_feature in ['subject_id', 'ts', 'label']:\n",
    "    dmy_cols.remove(unused_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df['subject_id'] == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.index[dmy_df['subject_id'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.iloc[dmy_df.index[dmy_df['subject_id'] == 4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Encoding categories\n",
    "\n",
    "Converting the categorical feature `Var4` into one hot encoded columns, so that it can be used by the neural networks and by embedding layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "~Encode each row's categorical value:~\n",
    "\n",
    "One hot encode the categorical feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# dmy_df['Var4'], enum_dict = du.embedding.enum_categorical_feature(dmy_df, feature='Var4',\n",
    "#                                                                   nan_value=0, forbidden_digit=0)\n",
    "# dmy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x1 = pd.get_dummies(dmy_df, columns=['Var4'])\n",
    "x1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x2 = pd.get_dummies(dmy_df, columns=['Var4'], sparse=True)\n",
    "x2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "x2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df, ohe_columns = du.data_processing.one_hot_encoding_dataframe(dmy_df, columns='Var4', \n",
    "                                                                    join_rows=False, \n",
    "                                                                    get_new_column_names=True, \n",
    "                                                                    inplace=True)\n",
    "dmy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ohe_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Joining the rows that have the same identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df = du.embedding.join_repeated_rows(dmy_df, id_columns=['subject_id', 'ts'])\n",
    "dmy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Normalizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_norm_df, mean, std = du.data_processing.normalize_data(dmy_df, id_columns=['subject_id', 'ts'],\n",
    "                                                           see_progress=False, get_stats=True)\n",
    "dmy_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# dmy_norm_df, mean, std = du.data_processing.normalize_data(dmy_df, id_columns=['subject_id', 'ts'],\n",
    "#                                                            categ_columns=['Var4'], see_progress=False,\n",
    "#                                                            get_stats=True)\n",
    "# dmy_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# dmy_norm_df, mean, std = du.data_processing.normalize_data(dmy_df, id_columns=['subject_id', 'ts'],\n",
    "#                                                            columns_to_normalize=False,\n",
    "#                                                            columns_to_normalize_categ=('Var4', ['Var0', 'Var1', 'Var2', 'Var3']), \n",
    "#                                                            see_progress=False, get_stats=True)\n",
    "# dmy_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# dmy_norm_df, mean, std = du.data_processing.normalize_data(dmy_df, id_columns=['subject_id', 'ts'],\n",
    "#                                                            columns_to_normalize=False,\n",
    "#                                                            columns_to_normalize_categ=('Var4', 'Var0'), \n",
    "#                                                            see_progress=False, get_stats=True)\n",
    "# dmy_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "stats = dict()\n",
    "for key, _ in mean.items():\n",
    "    stats[key] = dict()\n",
    "    stats[key]['mean'] = mean[key]\n",
    "    stats[key]['std'] = std[key]\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_norm_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Padding\n",
    "\n",
    "Pad the data so that all sequences have the same length (so that it can be converted to a PyTorch tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "padding_value = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "seq_len_dict = du.padding.get_sequence_length_dict(dmy_norm_df, id_column='subject_id', ts_column='ts')\n",
    "seq_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "data = du.padding.dataframe_to_padded_tensor(dmy_norm_df, seq_len_dict=seq_len_dict,\n",
    "                                             id_column='subject_id', padding_value=padding_value)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_perm = data.permute(1, 0, 2)\n",
    "data_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_perm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_perm[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset = du.datasets.Time_Series_Dataset(dmy_norm_df, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Separating into train and validation sets\n",
    "\n",
    "Since this notebook is only for experimentation purposes, with a very small dummy dataset, we'll not be using a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "batch_size = 32                                 # Number of patients in a mini batch\n",
    "n_epochs = 100                                  # Number of epochs\n",
    "lr = 0.001                                      # Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Separation in train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Get the train and validation sets data loaders, which will allow loading batches\n",
    "train_dataloader, val_dataloader, _ = du.machine_learning.create_train_sets(dataset, test_train_ratio=0, \n",
    "                                                                            validation_ratio=0.25,\n",
    "                                                                            batch_size=4, get_indeces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(iter(val_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Models testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Vanilla LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "\n",
    "\n",
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_layers = 2                                  # Number of LSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = Models.VanillaLSTM(n_inputs-3, n_hidden, n_outputs, n_layers, p_dropout)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               padding_value=padding_value, do_test=False, log_comet_ml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader, \n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value, \n",
    "                                                   output_rounded=False, set_name='test', \n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "### LSTM with embedding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_layers = 2                                  # Number of LSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout\n",
    "embed_features = [du.search_explore.find_col_idx(dmy_norm_df, col) for col in ohe_columns] # Indeces fo the features to be emebedded\n",
    "embed_features.sort()\n",
    "embedding_dim = 2                             # Number of outputs of the embedding layr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "embed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Models.VanillaLSTM(n_inputs-3, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                           embed_features=embed_features, embedding_dim=embedding_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.n_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(model.lstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# model = du.deep_learning.train(model, train_dataloader_df, val_dataloader_df, seq_len_dict=seq_len_dict,\n",
    "#                                batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "#                                padding_value=padding_value, do_test=False, log_comet_ml=False,\n",
    "#                                already_embedded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               padding_value=padding_value, do_test=False, log_comet_ml=False,\n",
    "                               already_embedded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.lstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader, \n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value, \n",
    "                                                   output_rounded=False, set_name='test', \n",
    "                                                   already_embedded=False,\n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "### LSTM with embedding layers and time interval handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Adding the time difference feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df['delta_ts'] = dmy_df.groupby('subject_id').ts.diff()\n",
    "dmy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Normalizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "dmy_norm_df = du.data_processing.normalize_data(dmy_df, id_columns=['subject_id', 'ts'],\n",
    "                                                see_progress=False)\n",
    "dmy_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_norm_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Imputation\n",
    "\n",
    "Replace the missing time difference values with the mean (zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_norm_df = du.data_processing.missing_values_imputation(dmy_norm_df, method='zero')\n",
    "dmy_norm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Padding\n",
    "\n",
    "Pad the data so that all sequences have the same length (so that it can be converted to a PyTorch tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "padding_value = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "seq_len_dict = du.padding.get_sequence_length_dict(dmy_norm_df, id_column='subject_id', ts_column='ts')\n",
    "seq_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "data = du.padding.dataframe_to_padded_tensor(dmy_norm_df, seq_len_dict=seq_len_dict,\n",
    "                                             id_column='subject_id', padding_value=padding_value)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset = du.datasets.Time_Series_Dataset(dmy_norm_df, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Separating into train and validation sets\n",
    "\n",
    "Since this notebook is only for experimentation purposes, with a very small dummy dataset, we'll not be using a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "batch_size = 32                                 # Number of patients in a mini batch\n",
    "n_epochs = 100                                  # Number of epochs\n",
    "lr = 0.001                                      # Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Separation in train and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Get the train and validation sets data loaders, which will allow loading batches\n",
    "train_dataloader, val_dataloader, _ = du.machine_learning.create_train_sets(dataset, test_train_ratio=0, \n",
    "                                                                            validation_ratio=0.25,\n",
    "                                                                            batch_size=4, get_indeces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(iter(val_dataloader))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_layers = 2                                  # Number of LSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout\n",
    "embed_features = [du.search_explore.find_col_idx(dmy_norm_df, col) for col in ohe_columns] # Indeces fo the features to be emebedded\n",
    "embed_features.sort()\n",
    "embedding_dim = 2                             # Number of outputs of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = Models.VanillaLSTM(n_inputs-3, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                           embed_features=embed_features, embedding_dim=embedding_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               padding_value=padding_value, do_test=False, log_comet_ml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader, \n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value, \n",
    "                                                   output_rounded=False, set_name='test', \n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "toc-hr-collapsed": true
   },
   "source": [
    "### T-LSTM\n",
    "\n",
    "Implementation of the [_Patient Subtyping via Time-Aware LSTM Networks_](http://biometrics.cse.msu.edu/Publications/MachineLearning/Baytasetal_PatientSubtypingViaTimeAwareLSTMNetworks.pdf) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_rnn_layers = 4                              # Number of TLSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout\n",
    "embed_features = [du.search_explore.find_col_idx(dmy_norm_df, col) for col in ohe_columns] # Indeces fo the features to be emebedded\n",
    "embed_features.sort()\n",
    "embedding_dim = 2                             # Number of outputs of the embedding layr\n",
    "# delta_ts_col = du.search_explore.find_col_idx(dmy_norm_df, 'delta_ts')   # Number of the delta_ts column\n",
    "elapsed_time = 'small'                                                   # Indicates if the elapsed time between events is small or long; influences how to discount elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dmy_norm_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "embed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Models.TLSTM(n_inputs-4, n_hidden, n_outputs, n_rnn_layers, p_dropout,\n",
    "                     embed_features=embed_features, embedding_dim=embedding_dim, \n",
    "                     elapsed_time=elapsed_time)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.weight_ih.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.rnn_layers[1].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               padding_value=padding_value, do_test=False, log_comet_ml=False,\n",
    "                               is_custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader, \n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value, \n",
    "                                                   output_rounded=False, set_name='test', \n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### MF1-LSTM\n",
    "\n",
    "Implementation of the [_Predicting healthcare trajectories from medical records: A deep learning approach_](https://doi.org/10.1016/j.jbi.2017.04.001) paper, time decay version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### MF2-LSTM\n",
    "\n",
    "Implementation of the [_Predicting healthcare trajectories from medical records: A deep learning approach_](https://doi.org/10.1016/j.jbi.2017.04.001) paper, parametric time version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Deep Care with parametric time\n",
    "\n",
    "Implementation of the [_Predicting healthcare trajectories from medical records: A deep learning approach_](https://doi.org/10.1016/j.jbi.2017.04.001) paper, full parametric time version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eicu-mortality-prediction",
   "language": "python",
   "name": "eicu-mortality-prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
