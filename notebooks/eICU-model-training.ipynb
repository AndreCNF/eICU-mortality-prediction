{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eICU Model training\n",
    "---\n",
    "\n",
    "Training models on the preprocessed the eICU dataset from MIT, which has data from over 139k patients collected in the US."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                  # os handles directory/workspace changes\n",
    "import comet_ml                            # Comet.ml can log training metrics, parameters, do version control and parameter optimization\n",
    "import torch                               # PyTorch to create and apply deep learning models\n",
    "import xgboost as xgb                      # Gradient boosting trees models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, roc_auc_score\n",
    "import joblib                              # Save scikit-learn models in disk\n",
    "from datetime import datetime              # datetime to use proper date and time formats\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging packages\n",
    "import pixiedust                           # Debugging in Jupyter Notebook cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the parquet dataset files\n",
    "data_path = 'data/eICU/cleaned/'\n",
    "# Path to the code files\n",
    "project_path = 'code/eICU-mortality-prediction/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the scripts directory\n",
    "os.chdir(\"../../scripts/\")\n",
    "import Models                              # Machine learning models\n",
    "# Change to parent directory (presumably \"Documents\")\n",
    "os.chdir(\"../../..\")\n",
    "# import modin.pandas as pd                  # Optimized distributed version of Pandas\n",
    "import pandas as pd                        # Pandas to load and handle the data\n",
    "import data_utils as du                    # Data science and machine learning relevant methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "du.set_pandas_library(lib='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allow pandas to show more columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 3000)\n",
    "pd.set_option('display.max_rows', 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "execution": {
     "iopub.execute_input": "2020-03-25T21:31:33.019976Z",
     "iopub.status.busy": "2020-03-25T21:31:33.019468Z",
     "iopub.status.idle": "2020-03-25T21:31:33.024337Z",
     "shell.execute_reply": "2020-03-25T21:31:33.023511Z",
     "shell.execute_reply.started": "2020-03-25T21:31:33.019939Z"
    }
   },
   "outputs": [],
   "source": [
    "stream_dtypes = open(f'{data_path}eICU_dtype_dict.yml', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "execution": {
     "iopub.execute_input": "2020-03-25T21:31:33.130183Z",
     "iopub.status.busy": "2020-03-25T21:31:33.129425Z",
     "iopub.status.idle": "2020-03-25T21:31:33.449639Z",
     "shell.execute_reply": "2020-03-25T21:31:33.448658Z",
     "shell.execute_reply.started": "2020-03-25T21:31:33.130144Z"
    }
   },
   "outputs": [],
   "source": [
    "dtype_dict = yaml.load(stream_dtypes, Loader=yaml.FullLoader)\n",
    "dtype_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window_h = 24                         # Number of hours on which we want to predict mortality\n",
    "id_column = 'patientunitstayid'            # Name of the sequence ID column\n",
    "ts_column = 'ts'                           # Name of the timestamp column\n",
    "n_inputs = 2093                            # Number of input features\n",
    "n_outputs = 1                              # Number of outputs\n",
    "padding_value = 999999                     # Padding value used to fill in sequences up to the maximum sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_ratio = 0.25                    # Percentage of the data which will be used as a test set\n",
    "validation_ratio = 0.1                     # Percentage of the data from the training set which is used for validation purposes\n",
    "batch_size = 32                            # Number of unit stays in a mini batch\n",
    "n_epochs = 10                              # Number of epochs\n",
    "lr = 0.001                                 # Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eICU_df = du.data_processing.load_chunked_data(file_name='eICU', n_chunks=8,\n",
    "                                               data_path=f'{data_path}normalized/ohe/', dtypes=dtype_dict)\n",
    "eICU_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of unit stays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eICU_df[id_column].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eICU_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eICU_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(eICU_df.columns) != n_inputs:\n",
    "    n_inputs = len(eICU_df.columns)\n",
    "    print(f'Changed the number of inputs to {n_inputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eICU_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the label\n",
    "\n",
    "Define the label column considering the desired time window on which we want to predict mortality (0, 24h, 48h, 72h, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "eICU_df['label'] = eICU_df.death_ts - eICU_df.ts <= time_window_h * 60\n",
    "eICU_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the now unneeded `death_ts` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eICU_df.drop(columns=death_ts, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "Pad the data so that all sequences have the same length (so that it can be converted to a PyTorch tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "seq_len_dict = du.padding.get_sequence_length_dict(eICU_df, id_column=id_column, ts_column=ts_column)\n",
    "seq_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "data = du.padding.dataframe_to_padded_tensor(eICU_df, seq_len_dict=seq_len_dict,\n",
    "                                             id_column=id_column, padding_value=padding_value)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset = du.datasets.Time_Series_Dataset(eICU_df, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating into train and validation sets\n",
    "\n",
    "Since this notebook is only for experimentation purposes, with a very small dummy dataset, we'll not be using a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Get the train and validation sets data loaders, which will allow loading batches\n",
    "train_dataloader, val_dataloader, test_dataloader = du.machine_learning.create_train_sets(dataset,\n",
    "                                                                                          test_train_ratio=test_train_ratio,\n",
    "                                                                                          validation_ratio=validation_ratio,\n",
    "                                                                                          batch_size=batch_size,\n",
    "                                                                                          get_indeces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(iter(val_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(test_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with one hot encoded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtracting 1 because of the removed label column, which was before these columns\n",
    "embed_features = [[du.search_explore.find_col_idx(eICU_df, col)-2 for col in feat_list]\n",
    "                   for feat_list in embed_features_names]\n",
    "embed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embeddings = []\n",
    "[n_embeddings.append(len(feat_list) + 1) for feat_list in embed_features]\n",
    "n_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] Join the columns that in fact belong to the same concept, such as the drughiclseqno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM with embedding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_layers = 2                                  # Number of LSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout\n",
    "embed_features = [du.search_explore.find_col_idx(dmy_norm_df, col) for col in ohe_columns] # Indeces fo the features to be emebedded\n",
    "embed_features.sort()\n",
    "embedding_dim = 2                             # Number of outputs of the embedding layr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Models.VanillaLSTM(n_inputs-3, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                           embed_features=embed_features, embedding_dim=embedding_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.lstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = du.deep_learning.train(model, train_dataloader_df, val_dataloader_df, seq_len_dict=seq_len_dict,\n",
    "#                                batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "#                                padding_value=padding_value, do_test=False, log_comet_ml=False,\n",
    "#                                already_embedded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               padding_value=padding_value, do_test=False, log_comet_ml=False,\n",
    "                               already_embedded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.lstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader,\n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value,\n",
    "                                                   output_rounded=False, set_name='test',\n",
    "                                                   already_embedded=False,\n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM with embedding layers and time interval handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding the time difference feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_df['delta_ts'] = dmy_df.groupby('subject_id').ts.diff()\n",
    "dmy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_norm_df = du.data_processing.normalize_data(dmy_df, id_columns=['subject_id', 'ts'],\n",
    "                                                see_progress=False)\n",
    "dmy_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_norm_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding\n",
    "\n",
    "Pad the data so that all sequences have the same length (so that it can be converted to a PyTorch tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_value = 999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len_dict = du.padding.get_sequence_length_dict(dmy_norm_df, id_column='subject_id', ts_column='ts')\n",
    "seq_len_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = du.padding.dataframe_to_padded_tensor(dmy_norm_df, seq_len_dict=seq_len_dict,\n",
    "                                             id_column='subject_id', padding_value=padding_value)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = du.datasets.Time_Series_Dataset(dmy_norm_df, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separating into train and validation sets\n",
    "\n",
    "Since this notebook is only for experimentation purposes, with a very small dummy dataset, we'll not be using a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Get the train and validation sets data loaders, which will allow loading batches\n",
    "train_dataloader, val_dataloader, test_dataloader = du.machine_learning.create_train_sets(dataset,\n",
    "                                                                                          test_train_ratio=test_train_ratio,\n",
    "                                                                                          validation_ratio=validation_ratio,\n",
    "                                                                                          batch_size=batch_size,\n",
    "                                                                                          get_indeces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "next(iter(val_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(test_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_layers = 2                                  # Number of LSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout\n",
    "embed_features = [du.search_explore.find_col_idx(dmy_norm_df, col) for col in ohe_columns] # Indeces fo the features to be emebedded\n",
    "embed_features.sort()\n",
    "embedding_dim = 2                             # Number of outputs of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Models.VanillaLSTM(n_inputs-3, n_hidden, n_outputs, n_layers, p_dropout,\n",
    "                           embed_features=embed_features, embedding_dim=embedding_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               padding_value=padding_value, do_test=False, log_comet_ml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader,\n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value,\n",
    "                                                   output_rounded=False, set_name='test',\n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.TLSTM, df=dmy_norm_df,\n",
    "                                                                              config_name='TLSTM_hyperparameter_optimization_config.yaml',\n",
    "                                                                              comet_ml_api_key='jiDa6SsGNoyddaLPZESuAO6qi',\n",
    "                                                                              comet_ml_project_name='models-dummy-tests',\n",
    "                                                                              comet_ml_workspace='andrecnf',\n",
    "                                                                              n_inputs=n_inputs-4, id_column='subject_id',\n",
    "                                                                              label_column='label', inst_column='ts',\n",
    "                                                                              n_outputs=1, model_type='multivariate_rnn',\n",
    "                                                                              is_custom=True, models_path='models/', array_param=None,\n",
    "                                                                              config_path='notebooks/sandbox/', var_seq=True,\n",
    "                                                                              clip_value=0.5, padding_value=padding_value,\n",
    "                                                                              batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                              lr=lr, test_train_ratio=0, validation_ratio=0.25,\n",
    "                                                                              comet_ml_save_model=True, embed_features=embed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-LSTM\n",
    "\n",
    "Implementation of the [_Patient Subtyping via Time-Aware LSTM Networks_](http://biometrics.cse.msu.edu/Publications/MachineLearning/Baytasetal_PatientSubtypingViaTimeAwareLSTMNetworks.pdf) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_rnn_layers = 4                              # Number of TLSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout\n",
    "embed_features = [du.search_explore.find_col_idx(dmy_norm_df, col) for col in ohe_columns] # Indeces fo the features to be emebedded\n",
    "embed_features.sort()\n",
    "embedding_dim = 2                             # Number of outputs of the embedding layr\n",
    "# delta_ts_col = du.search_explore.find_col_idx(dmy_norm_df, 'delta_ts')   # Number of the delta_ts column\n",
    "elapsed_time = 'small'                                                   # Indicates if the elapsed time between events is small or long; influences how to discount elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_norm_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Models.TLSTM(n_inputs-4, n_hidden, n_outputs, n_rnn_layers, p_dropout,\n",
    "                     embed_features=embed_features, embedding_dim=embedding_dim,\n",
    "                     elapsed_time=elapsed_time)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.weight_ih.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[1].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               padding_value=padding_value, do_test=False, log_comet_ml=False,\n",
    "                               is_custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader,\n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value,\n",
    "                                                   output_rounded=False, set_name='test',\n",
    "                                                   is_custom=True,\n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.TLSTM, df=dmy_norm_df,\n",
    "                                                                              config_name='TLSTM_hyperparameter_optimization_config.yaml',\n",
    "                                                                              comet_ml_api_key='jiDa6SsGNoyddaLPZESuAO6qi',\n",
    "                                                                              comet_ml_project_name='models-dummy-tests',\n",
    "                                                                              comet_ml_workspace='andrecnf',\n",
    "                                                                              n_inputs=n_inputs-4, id_column='subject_id',\n",
    "                                                                              label_column='label', inst_column='ts',\n",
    "                                                                              n_outputs=1, model_type='multivariate_rnn',\n",
    "                                                                              is_custom=True, models_path='models/', array_param=None,\n",
    "                                                                              config_path='notebooks/sandbox/', var_seq=True,\n",
    "                                                                              clip_value=0.5, padding_value=padding_value,\n",
    "                                                                              batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                              lr=lr, test_train_ratio=0, validation_ratio=0.25,\n",
    "                                                                              comet_ml_save_model=True, embed_features=embed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MF1-LSTM\n",
    "\n",
    "Implementation of the [_Predicting healthcare trajectories from medical records: A deep learning approach_](https://doi.org/10.1016/j.jbi.2017.04.001) paper, time decay version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_rnn_layers = 4                              # Number of TLSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout\n",
    "embed_features = [du.search_explore.find_col_idx(dmy_norm_df, col) for col in ohe_columns] # Indeces fo the features to be emebedded\n",
    "embed_features.sort()\n",
    "embedding_dim = 2                             # Number of outputs of the embedding layr\n",
    "# delta_ts_col = du.search_explore.find_col_idx(dmy_norm_df, 'delta_ts')   # Number of the delta_ts column\n",
    "elapsed_time = 'small'                                                   # Indicates if the elapsed time between events is small or long; influences how to discount elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_norm_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Models.MF1LSTM(n_inputs-4, n_hidden, n_outputs, n_rnn_layers, p_dropout,\n",
    "                       embed_features=embed_features, embedding_dim=embedding_dim,\n",
    "                       elapsed_time=elapsed_time)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.weight_ih.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[1].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               ModelClass=Models.MF1LSTM, padding_value=padding_value, do_test=False,\n",
    "                               log_comet_ml=False, is_custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader,\n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value,\n",
    "                                                   output_rounded=False, set_name='test',\n",
    "                                                   is_custom=True,\n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.MF1LSTM, df=dmy_norm_df,\n",
    "                                                                              config_name='TLSTM_hyperparameter_optimization_config.yaml',\n",
    "                                                                              comet_ml_api_key='jiDa6SsGNoyddaLPZESuAO6qi',\n",
    "                                                                              comet_ml_project_name='models-dummy-tests',\n",
    "                                                                              comet_ml_workspace='andrecnf',\n",
    "                                                                              n_inputs=n_inputs-4, id_column='subject_id',\n",
    "                                                                              label_column='label', inst_column='ts',\n",
    "                                                                              n_outputs=1, model_type='multivariate_rnn',\n",
    "                                                                              is_custom=True, models_path='models/', array_param=None,\n",
    "                                                                              config_path='notebooks/sandbox/', var_seq=True,\n",
    "                                                                              clip_value=0.5, padding_value=padding_value,\n",
    "                                                                              batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                              lr=lr, test_train_ratio=0, validation_ratio=0.25,\n",
    "                                                                              comet_ml_save_model=True, embed_features=embed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MF2-LSTM\n",
    "\n",
    "Implementation of the [_Predicting healthcare trajectories from medical records: A deep learning approach_](https://doi.org/10.1016/j.jbi.2017.04.001) paper, parametric time version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ids = dmy_norm_df.subject_id.nunique()      # Total number of sequences\n",
    "n_inputs = len(dmy_norm_df.columns)           # Number of input features\n",
    "n_hidden = 10                                 # Number of hidden units\n",
    "n_outputs = 1                                 # Number of outputs\n",
    "n_rnn_layers = 4                              # Number of TLSTM layers\n",
    "p_dropout = 0.2                               # Probability of dropout\n",
    "embed_features = [du.search_explore.find_col_idx(dmy_norm_df, col) for col in ohe_columns] # Indeces fo the features to be emebedded\n",
    "embed_features.sort()\n",
    "embedding_dim = 2                             # Number of outputs of the embedding layr\n",
    "# delta_ts_col = du.search_explore.find_col_idx(dmy_norm_df, 'delta_ts')   # Number of the delta_ts column\n",
    "elapsed_time = 'small'                                                   # Indicates if the elapsed time between events is small or long; influences how to discount elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmy_norm_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Models.MF2LSTM(n_inputs-4, n_hidden, n_outputs, n_rnn_layers, p_dropout,\n",
    "                       embed_features=embed_features, embedding_dim=embedding_dim,\n",
    "                       elapsed_time=elapsed_time)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.weight_ih.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[0].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn_layers[1].cell.delta_ts_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = du.deep_learning.train(model, train_dataloader, val_dataloader, seq_len_dict=seq_len_dict,\n",
    "                               batch_size=batch_size, n_epochs=n_epochs, lr=lr, models_path='models/',\n",
    "                               ModelClass=Models.MF2LSTM, padding_value=padding_value, do_test=False,\n",
    "                               log_comet_ml=False, is_custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model.embed_layers.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, metrics = du.deep_learning.model_inference(model, dataloader=val_dataloader,\n",
    "                                                   metrics=['loss', 'accuracy', 'AUC'],\n",
    "                                                   seq_len_dict=seq_len_dict, padding_value=padding_value,\n",
    "                                                   output_rounded=False, set_name='test',\n",
    "                                                   is_custom=True,\n",
    "                                                   cols_to_remove=[du.search_explore.find_col_idx(dmy_norm_df, feature)\n",
    "                                                                   for feature in ['subject_id', 'ts']])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "##### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    val_loss_min, exp_name_min = du.machine_learning.optimize_hyperparameters(Models.MF2LSTM, df=dmy_norm_df,\n",
    "                                                                              config_name='TLSTM_hyperparameter_optimization_config.yaml',\n",
    "                                                                              comet_ml_api_key='jiDa6SsGNoyddaLPZESuAO6qi',\n",
    "                                                                              comet_ml_project_name='models-dummy-tests',\n",
    "                                                                              comet_ml_workspace='andrecnf',\n",
    "                                                                              n_inputs=n_inputs-4, id_column='subject_id',\n",
    "                                                                              label_column='label', inst_column='ts',\n",
    "                                                                              n_outputs=1, model_type='multivariate_rnn',\n",
    "                                                                              is_custom=True, models_path='models/', array_param=None,\n",
    "                                                                              config_path='notebooks/sandbox/', var_seq=True,\n",
    "                                                                              clip_value=0.5, padding_value=padding_value,\n",
    "                                                                              batch_size=batch_size, n_epochs=n_epochs,\n",
    "                                                                              lr=lr, test_train_ratio=0, validation_ratio=0.25,\n",
    "                                                                              comet_ml_save_model=True, embed_features=embed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_hyperparam_optim:\n",
    "    exp_name_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapting the data to XGBoost and Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sckt_eICU_df = eICU_df.copy()\n",
    "sckt_eICU_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical columns to string type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sckt_eICU_df.race = sckt_eICU_df.race.astype(str)\n",
    "sckt_eICU_df.ajcc_pathologic_tumor_stage = sckt_eICU_df.ajcc_pathologic_tumor_stage.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encode categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sckt_eICU_df, new_cols= du.data_processing.one_hot_encoding_dataframe(sckt_eICU_df, columns=['race', 'ajcc_pathologic_tumor_stage'],\n",
    "                                                                      clean_name=False, clean_missing_values=False,\n",
    "                                                                      has_nan=False, join_rows=False,\n",
    "                                                                      get_new_column_names=True, inplace=True)\n",
    "new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sckt_eICU_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the ID column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sckt_eICU_df = sckt_eICU_df.drop(columns='sample_id')\n",
    "sckt_eICU_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a PyTorch tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sckt_eICU_tsr = torch.from_numpy(sckt_eICU_df.to_numpy())\n",
    "sckt_eICU_tsr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = du.datasets.Tabular_Dataset(sckt_eICU_tsr, sckt_eICU_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.label_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the train, validation and test sets data loaders, which will allow loading batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = du.machine_learning.create_train_sets(dataset, test_train_ratio=0.2, validation_ratio=0.1,\n",
    "                                                                                          batch_size=len(dataset), get_indeces=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the full tensors with all the data from each set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "val_features, val_labels = next(iter(val_dataloader))\n",
    "test_features, test_labels = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normal training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = eICU_df.tumor_type_label.nunique()    # Number of classes\n",
    "lr = 0.001                                      # Learning rate\n",
    "objective = 'multi:softmax'                     # Objective function to minimize (in this case, softmax)\n",
    "eval_metric = 'mlogloss'                        # Metric to analyze (in this case, multioutput negative log likelihood loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=objective, eval_metric='mlogloss', learning_rate=lr,\n",
    "                              num_class=n_class, random_state=du.random_seed, seed=du.random_seed)\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with early stopping (stops training if the evaluation metric doesn't improve on 5 consequetive iterations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(train_features, train_labels, early_stopping_rounds=5, eval_set=[(val_features, val_labels)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current day and time to attach to the saved model's name\n",
    "current_datetime = datetime.now().strftime('%d_%m_%Y_%H_%M')\n",
    "# Filename and path where the model will be saved\n",
    "model_filename = f'{models_path}xgb/checkpoint_{current_datetime}.model'\n",
    "# Save the model\n",
    "joblib.dump(xgb_model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_model = joblib.load(f'{models_path}xgb/checkpoint_16_12_2019_11_39.model')\n",
    "xgb_model = joblib.load(model_filename)\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train until the best iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective=objective, eval_metric='mlogloss', learning_rate=lr,\n",
    "                              num_class=n_class, random_state=du.random_seed, seed=du.random_seed)\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.fit(train_features, train_labels, early_stopping_rounds=5, num_boost_round=xgb_model.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(test_labels, pred)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(test_labels, pred, average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = xgb_model.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = log_loss(test_labels, pred_proba)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(test_labels, pred_proba, multi_class='ovr', average='weighted')\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normal training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_class = 'multinomial'\n",
    "solver = 'lbfgs'\n",
    "penalty = 'l2'\n",
    "C = 1\n",
    "max_iter = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(multi_class=multi_class, solver=solver, penalty=penalty, C=C, max_iter=max_iter, random_state=du.random_seed)\n",
    "logreg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current day and time to attach to the saved model's name\n",
    "current_datetime = datetime.now().strftime('%d_%m_%Y_%H_%M')\n",
    "# Filename and path where the model will be saved\n",
    "model_filename = f'{models_path}logreg/checkpoint_{current_datetime}.model'\n",
    "# Save the model\n",
    "joblib.dump(logreg_model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg_model = joblib.load(f'{models_path}logreg/checkpoint_16_12_2019_02_27.model')\n",
    "logreg_model = joblib.load(model_filename)\n",
    "logreg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = logreg_model.score(test_features, test_labels)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = logreg_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(test_labels, pred, average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = logreg_model.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = log_loss(test_labels, pred_proba)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(test_labels, pred_proba, multi_class='ovr', average='weighted')\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normal training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_function_shape = 'ovo'\n",
    "C = 1\n",
    "kernel = 'rbf'\n",
    "max_iter = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel=kernel, decision_function_shape=decision_function_shape, C=C,\n",
    "                max_iter=max_iter, probability=True, random_state=du.random_seed)\n",
    "svm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current day and time to attach to the saved model's name\n",
    "current_datetime = datetime.now().strftime('%d_%m_%Y_%H_%M')\n",
    "# Filename and path where the model will be saved\n",
    "model_filename = f'{models_path}svm/checkpoint_{current_datetime}.model'\n",
    "# Save the model\n",
    "joblib.dump(svm_model, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_model = joblib.load(f'{models_path}svm/checkpoint_16_12_2019_05_51.model')\n",
    "svm_model = joblib.load(model_filename)\n",
    "svm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = logreg_model.score(test_features, test_labels)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = logreg_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(test_labels, pred, average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = logreg_model.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = log_loss(test_labels, pred_proba)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(test_labels, pred_proba, multi_class='ovr', average='weighted')\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter optimization"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
